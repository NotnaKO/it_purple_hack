# Платформа ценнообразования Авито

## Решение команды Calculus Enjoyers
`Backend` - _Golang_

`Frontend` - _HTML_/_CSS_/_JavaScript_

`Test` - _Python_

# Содержание
- [Платформа ценнообразования Авито](#платформа-ценнообразования-авито)
  - [Решение команды Calculus Enjoyers](#решение-команды-calculus-enjoyers)
- [Содержание](#содержание)
- [Technologies \& Tools](#technologies--tools)
- [Фишки](#фишки)
- [UI](#ui)
- [Архитектура решения кейса](#архитектура-решения-кейса)
  - [Диаграмма работы сервисов](#диаграмма-работы-сервисов)
  - [Оптимизации](#оптимизации)
  - [Алгоритм get\_price (подробно)](#алгоритм-get_price-подробно)
- [Не функциональные требования:](#не-функциональные-требования)
- [Тестирование продукта](#тестирование-продукта)
- [Инструкция запуска](#инструкция-запуска)

# Technologies & Tools
1) Языки: _Golang_, _Python_, _JavaScript_
2) Системы хранения данных: _Redis_, _PostgreSQL_
3) Конфиги: _JSON_, _YAML_
4) Скрипты: _bash_
5) Сетевые протоколы, утилиты: _https_, _curl_

# Фишки
1) `User Interface` для аналитиков с возможностью точечного изменения значений в существующих матрицах цен. Удобное изменение baseline и discount матриц позволяет корректировать storage, не закрывая браузер.
2) Сервис сбора метрик работы сервиса, данные выводятся напрямую аналитику.
3) Оптимизированная работа с таблицами возврата цен.
4) Использование `Redis` system для быстрого ответ пользователю.

# UI
TODO описание сервиса\

# Архитектура решения кейса
1) `MultiUser price_retrieval` - сервер, позволяющий получать стоймость по локации и категории для пользователей.
2) `MultiUser price_management` - сервер, напрямую взаимодействующий с таблицами PostgreSQL, созданный для ответов на запросы retrieval сервера, а также API для аналитиков, позволяющий изменять цены в таблицах, обновлять storage (baseline и discount таблицы).
3) `MultiUser price_ui` - веб-приложение для аналитиков с графическим интерфейсом, который позволяет проводить манипуляции с текущими таблицами и получать информацию(подробнее [выше](#ui)).

## Диаграмма работы сервисов
<img src = "photo.jpg">

## Оптимизации
1) Используем `кеширование` на пути дерева (категорий, локаций), оптимизация отлично себя показала на нагрузочных тестах увеличив `RPS` с 85 до 350, кеш мы чистим при изменении исходной таблицы или раз в полчаса как написано ниже (с указанием причины). Кроме того, `дерево поиска` сначала проверяет приоритетные таблицы (с дисконт-сегментами для _user_id_, которые мы передаем из UI в формате JSON).

2) `Разделение таблиц`, эта оптимизация дает ускорение на больших обьемах данных, так как мы на каждый запрос поиск данных происходит в таблице размером на один-два порядка меньше, кроме того, часть самых частых запросов закешировано. В этой части мы делим таблицу по _microcategory_id_ (мотивация ниже), но также имеем возможность в два действия изменить _matrix_size_, и делить таблицу на _matrix_size_ элементов(сейчас тесты подобраны чтобы делить на интересные нам таблицы), в production на примерно равные таблицы и когда произведено много запросов `set_price` добавлено делаем перебалансировку в ночное время суток, когда трафик снижен до минимальных значений.

3) Для каждого пользователя мы знаем json|матрицу скидочных категориий, и мы его выгружаем при создании price_retrieval. Таким образом когда мы делаем запрос для пользователя мы уже за _O(1)_ знаем надо ли нам ходить в Discount сегмент или нет. Это достаточно сильная оптимизация, тк если для пользователя нет скидок, то и смысла ходить нет, а такое обычно у большинства.

4) Делаем легкий запрос без json:
Пользователь к нам всегда приходит с 3-4 параметрами microcategory, location, data_base_id, price, где data_base_id создан для удобства аналитиков чтобы не писать название какую матрицу они меняют и оптимизации величины запроса (в UI показывается json - matrix_name|id)

1) Используем `оптимизированные API`, наши сервера слушают пользователей `асинхронно`, поэтому имеется возможность в MultiUser обращения.

2) В качестве БД была использована _PostgreSQL_, как архитектура, бесплатная, простая к использованию и показывающая отличный `perfomance` в скорости работы относительно других OpenSource решений (выводы сделаны на основе графиков `YaTalks2023` и ресурсов `habr`)
   * _NoSQL_ требует больше памяти, хоть может быть выгоден для небольшого числа данных(как, например, у нас кеш для дерева), поэтому был сделан выбор в пользу SQL, чтобы соблюсти баланс скорости ко времени, также рассматриватривалась _MySQL_ (со схожими показателями).
   * Храненение данных в файлах, например, _JSON_ - очень медленный способ для такого обьема данных, и вставка в них работает за _O(длина файла)_, в отличии от предыдущих технологий.

## Алгоритм get_price (подробно)
1) Пользователь для получения цены задает запрос в сервис price_retriver, дальше мы идем по location_tree и microcategory_tree
2) Применяя алгоритм подьема с оптимизацией в виде кеширования на пути, те если мы получили ответ в i вершине, а начали в j, то кешируем все на пути с j по i.
3) Если вершина была в кеше, то возвращаем ответ и не поднимаемся дальше. В качестве кеша используем NoSQL систему Redis, тк она наиболее быстрая и идеально подходит для нашего дерева, в котором более 250'000 вершин. 
** Сам кеш мы чистим раз в N минут, которое передается через конфиг, чтобы убрать из системы старые вершины, в которые давно никто не ходил.
4) Если запрос не в кеше, мы его туда положим на всех вершинах в пути, в процессе возвращения ответа. А пока его там нет идем в connector, который кеширует дерево, и отправляет запрос в сервис, общающийся с DataBase.
5) Запрос попадает в сервис price_manager по ручке get_price, по https запросу, в котором передается всего 3 аргумента в запросе, так что тут тоже оптимизация в отличии неэффективного JSON
6) Внутри сервиса стоит наша главная оптимизация, тк исходная baseline_matrix очень большая, то и поиск по ней происходит долго, мы разбиваем ее на матрицы(которые тоже создаются в нашем сервисе) меньшего размера(в среднем если исходная матрица 250к строк, то дополнительные максимум 25к строк), сильно уменьшать и разбивать тоже плохо, тк над DataBase построена целая архитектура, которая требует памят, поэтому разбиваем на матрицы размера ~5-max(10)% от исходной матрицы(для тестовых матриц, для продовых в конфиге пишется matrix_size - максимальный размер матрицы).
7) Внутри самого сервиса мы однозначно умеем сопоставлять за _O(1)_ (за счет matrix naming) имя матрицы, в которую отправить запрос и таким образом мы имеем возможность для всех запросов поиска цены ходить в оперделенную матрицу.
**Разбитие на матрицы происходит по параметру _microcategory_id_, тк размерность дерева этого параметра~совпадает с размерностью дерева локаций а общая таблица представляет из себя их декартово произведение(в общем случае)
8) Как только к нам пришел элемент мы по цепочке как показано отдаем пользователю, все клиенты к обоим серверам подключаются асинхронно и с помощью стресс тестов мы смогли замерить RPS нашей системы. На всех этапах мы передаем небольшие запросы и лишь ответ пользователю собираем в виде JSON файла. Тк нам от price_management важно лишь получение цены, те 1 число.

# Не функциональные требования:
1) Сервис имеет все `возможности для масштабирования`, так как в нашей реализации несколько микросервисов связанны между собой исключительно по `API`, и не знают о внутреннем устройстве друг друга. Внутри сервисов нет "magic const" и захардкоженных map, вместо этого в `config` сервера пердаются названия _JSON_-файлов, где собрана основная ифнормация (при желании имеется возможность _JSON_ заменить на _YAML_|_SQL_|_NoSQL_), но для тестирования с небольшим числом userов это - идеальный вариант.
Также наши сервисы поддерживают `логгирование ошибок` при запросах и обработке, что может оказать существенную помощь в дебаге и масштабируемости на всех этапах использования нашего сервиса.

2) Сервис доступен почти `все время` за исключением нескольких минут на обновление таблиц пользователей и сброс кеша, что происходит редко по сравнению с количеством и частотой обрабатываемых запросов.

3) Сервис `устойчив` к потере датацентров, так как каждая функция берет таблицы и, если не получает ответ на запрос, переходит к другой, в этом месте имеем возможность вписать очередность запроса к `Data Center`.

4) Отдача как показывают `stress test` для 99.9% пользователей происходит `менее чем за 200мс`.

5) Сервис также крутится на машине, и любые ошибки его не убьют, тк они обрабатываются, а пользователю вернется лишь отсутствие ответа. В остальное время он доступен.

# Тестирование продукта
1) Командой были написаны stress тесты, которые представляют выбор из доступных _loc_id_ и _micro_id_ для больших baseline_matrix_table, discount_matrix_table, где размер baseline - 250'000 строк, а discount - 4000, также выбраны userid положены в _JSON_, чтобы была полноценная симуляция реального мира. На таких тестах при запоминании кеша происходит постепенный рост RPS с 300 до 500 с вышеперчисленными оптимизациями.
2) Детерменированные тесты, находящиеся в каждом сервисе, для провери корректности выдачи цены

# Инструкция запуска
Подробный README по запуску компоненты находится в каждой папке сервиса(названия соответсвующие)
1) price_retrieval - сервис получения цен для пользователей
2) price_management - сервис работы с базой данных, отвечающий за запросы аналитиков и выдаче по запросу get_price(тема кейса)
3) price_ui - сервис для аналитиков с GUI
4) data_base_generation - скрипты для создания тестовых DataBase(3 таблицы discount segmenta и 3 таблицы baseline_matrix для теста корректности, предоставленные в приложениях к кейсу)
